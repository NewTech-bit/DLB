{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99a5a92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d3ab274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    y = df['Category'] # y: 'Category' column\n",
    "    y = np.array(y)\n",
    "    \n",
    "    x = df['Message'] #x: 'Message' column\n",
    "    x = np.array(x)\n",
    "    \n",
    "    return df, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "81841d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, x, y = load_dataset('SPAM text message 20170820 - Data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd2fa5",
   "metadata": {},
   "source": [
    "## Pre-processing the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4df3e2a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category    0\n",
       "Message     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check NAs\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a32b2c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "26151e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3900\n",
      "1672\n"
     ]
    }
   ],
   "source": [
    "# check the split results\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a878beba",
   "metadata": {},
   "source": [
    "Machine learning algorithms and deep learning neural networks require that input and output variables are numbers.\n",
    "This means that categorical data must be encoded to numbers before we can use it to fit and evaluate a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ccce84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string in \"Category\" column to numerical binary encoding\n",
    "# reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "\n",
    "def label_encoder(data):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(data) # first fit lable encoder\n",
    "    enc = le.transform(data) # then transform labels to normalized encoding\n",
    "    return enc\n",
    "\n",
    "# y_test_enc = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "44d86728",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_enc = label_encoder(y_train)\n",
    "y_test_enc = label_encoder(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f1d901c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'ham' 'ham' 'ham' 'ham' 'ham' 'ham' 'ham' 'ham' 'ham' 'ham' 'ham'\n",
      " 'ham' 'ham' 'spam' 'ham' 'ham' 'ham' 'ham' 'ham']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# check the encoding results\n",
    "print(y_train[:20])\n",
    "print(y_train_enc[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdec4a3",
   "metadata": {},
   "source": [
    "## Tokenizing the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b1c89d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tfidf vectorization\n",
    "\n",
    "tfidfvectorizer = TfidfVectorizer(analyzer='word', stop_words='english')\n",
    "x_train_vect = tfidfvectorizer.fit_transform(x_train)\n",
    "x_test_vect = tfidfvectorizer.fit_transform(x_test)\n",
    "\n",
    "# tfidfvectorizer = TfidfVectorizer(analyzer='word', stop_words='english').fit(x_train)\n",
    "# x_train_vect = tfidfvectorizer.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3e93caa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Think I could stop by in like an hour or so? My roommate's looking to stock up for a trip\n"
     ]
    }
   ],
   "source": [
    "print(x_train[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c737d644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2221)\t0.4595576461719437\n",
      "  (0, 6907)\t0.42057032296489166\n",
      "  (0, 260)\t0.3977642008852706\n",
      "  (0, 943)\t0.35285068394560465\n",
      "  (0, 3629)\t0.33392519134134463\n",
      "  (0, 3634)\t0.31674844532481095\n",
      "  (0, 5025)\t0.3425956765507876\n",
      "  (1, 5087)\t0.564752315585174\n",
      "  (1, 4329)\t0.4556948018194525\n",
      "  (1, 6399)\t0.6880385669683886\n",
      "  (2, 5909)\t0.4598907748288949\n",
      "  (2, 1882)\t0.5577039025555312\n",
      "  (2, 5320)\t0.407962240134966\n",
      "  (2, 1557)\t0.5577039025555312\n",
      "  (3, 2032)\t0.5517950786382567\n",
      "  (3, 1762)\t0.4550182724182232\n",
      "  (3, 6157)\t0.39464721758363247\n",
      "  (3, 889)\t0.4179976347325351\n",
      "  (3, 5392)\t0.3975073759914889\n",
      "  (4, 3168)\t0.517613431846439\n",
      "  (4, 5675)\t0.6128580677760653\n",
      "  (4, 2864)\t0.40046755880208973\n",
      "  (4, 6864)\t0.44283976592107693\n"
     ]
    }
   ],
   "source": [
    "print(x_train_vect[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "76df111b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3900,)\n",
      "(3900, 7003)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8e1b7417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3900,)\n",
      "(3900,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_train_enc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fc8316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7449e546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14154654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be3ab729",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cc090ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x_train_vect.shape[1]\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aae797a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_sequential_model():\n",
    "#     seq_model = tf.keras.Sequential([\n",
    "#         Dense(64, input_dim=(1,), activation='relu'),\n",
    "#         Dense(32, activation='relu'),\n",
    "#         Dense(16, activation='relu'),\n",
    "#         Dense(1, activation='sigmoid')\n",
    "#     ])\n",
    "#     return seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b2a35565",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.Sequential()\n",
    "model.add(Dense(16, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "32d74a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 16)                112064    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 112,081\n",
      "Trainable params: 112,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d9fce4b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_2_input to have shape (7003,) but got array with shape (4183,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-c6fa2195c297>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_vect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     batch_size=100)\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/envs/clt/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m           \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m           steps_name='validation_steps')\n\u001b[0m\u001b[1;32m    722\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_symbolic_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/clt/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2649\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2651\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/clt/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    383\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    386\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_2_input to have shape (7003,) but got array with shape (4183,)"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_vect, y_train_enc,\n",
    "                    epochs=5,\n",
    "                    verbose=True,\n",
    "                    validation_data=(x_test_vect, y_test_enc),\n",
    "                    batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93323bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2948c35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd0369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1557d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
